# -*- coding: utf-8 -*-
"""E2.1.2-TimePerformances.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ByE9zLoJaO8EmHRqW3414NWncLxLu5_x
"""

import pandas as pd
#%pip install pm4py
import pm4py
from pm4py.objects.log.util import dataframe_utils
from pm4py.objects.conversion.log import converter as log_converter

# Load Event Log file in CSV

log_csv = pd.read_csv('https://raw.githubusercontent.com/paoloceravolo/PM-Regensburg/main/CallCenterLog.csv', sep=',')
log_csv = dataframe_utils.convert_timestamp_columns_in_df(log_csv)

# Sort by the timestamp column

log_csv = log_csv.sort_values('Start Date')

# Identify the case_id_key name (if not change it will simply be the mane of the coloumn)

log_csv.rename(columns={'Case ID': 'case:concept:name', 'Start Date': 'start_timestamp', 'End Date': 'time:timestamp', 'Activity': 'concept:name', 'Resource': 'org:resource'}, inplace=True) #change the name to a colum
parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'} # identify the case_id_key name (if not change it will simply be the mane of the coloumn)

profile = log_csv.groupby('case:concept:name').agg(
Activity = ('concept:name', 'count'),\
Activity_list = ('concept:name', lambda x: ', '.join(x)),\
# Count distinct values
Resource = ('org:resource', 'nunique'),\
# Delta of a column
Delta_start_time = ('start_timestamp', lambda x: x.max() - x.min()),\
)

pd.set_option('display.max_columns', 20) # print more columns 
print(profile.head())

#print(log_csv.start_timestamp)

# Convert time to datetime, not really needed in this example
log_csv.start_timestamp = pd.to_datetime(log_csv.start_timestamp)
log_csv['time:timestamp'] = pd.to_datetime(log_csv['time:timestamp'])

#print(log_csv.start_timestamp)
#print(log_csv)

event_log = log_converter.apply(log_csv, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)

#print(event_log)

# Import Throughput Time
from pm4py.statistics.traces.generic.log import case_statistics

all_case_durations = case_statistics.get_all_case_durations(event_log, parameters={
    case_statistics.Parameters.TIMESTAMP_KEY: "time:timestamp"})
median_case_duration = case_statistics.get_median_case_duration(event_log, parameters={
    case_statistics.Parameters.TIMESTAMP_KEY: "time:timestamp"
})

print(all_case_durations)
print('Median duration', median_case_duration)

# Plot Performance Spectrum

pm4py.view_performance_spectrum(event_log, ["Inbound Call", "Call Outbound", "Handle Case"], format="png")

# import Cycle Time and Waiting Time function
from pm4py.objects.log.util import interval_lifecycle
from pm4py.util import constants

# Event log enriched with time performance values 
enriched_log = interval_lifecycle.assign_lead_cycle_time(event_log, parameters={
    constants.PARAMETER_CONSTANT_START_TIMESTAMP_KEY: "start_timestamp", 
    constants.PARAMETER_CONSTANT_TIMESTAMP_KEY: "time:timestamp",
    "worktiming": [0, 24],
    "weekends": []})
    

from pm4py.algo.transformation.log_to_features import algorithm as log_to_features

data, feature_names = log_to_features.apply(enriched_log, parameters={"str_ev_attr": ["concept:name", "org:resource"], "str_tr_attr": [], "num_ev_attr": ["@@approx_bh_partial_cycle_time", "@@approx_bh_partial_lead_time",  "@@approx_bh_overall_wasted_time", "@@approx_bh_this_wasted_time", "@approx_bh_ratio_cycle_lead_time"], "num_tr_attr": [], "str_evsucc_attr": ["concept:name", "org:resource"]})
                                
# Print enriched_log
#print(enriched_log)
#print(type(enriched_log))

# Print features 
#print(feature_names)

# Convert Event log to Data Frame

data_log = log_converter.apply(enriched_log, variant=log_converter.Variants.TO_DATA_FRAME)

#print(data_log)
# Print specific columns
print(data_log[['case:concept:name', 'concept:name', '@@approx_bh_partial_cycle_time', '@@approx_bh_this_wasted_time', '@@approx_bh_partial_lead_time']])
# Print based on columns value  # Problems with Case 1144
print(data_log.loc[data_log['case:concept:name'] == 'Case 2559'])

profile = data_log.groupby('case:concept:name').agg(
Activity = ('concept:name', 'count'),\
Activity_list = ('concept:name', lambda x: ', '.join(x)),\
# Count distinct values
Resource = ('org:resource', 'nunique'),\
# Delta of a column
Delta_start_time = ('start_timestamp', lambda x: x.max() - x.min()),\
Cycle_Time = ('@@approx_bh_partial_cycle_time', 'max'),\
Wasted_Time = ('@@approx_bh_this_wasted_time', 'max'),\
Lead_Time = ('@@approx_bh_partial_lead_time', 'max'),\
)

pd.set_option('display.max_columns', 20) # print more columns 
print(profile.head())



# convert list in df
#df = pd.DataFrame(data, columns=feature_names)

#df['event:@@approx_bh_partial_lead_time'].describe()

#print(df['event:@@approx_bh_partial_lead_time'].mean())
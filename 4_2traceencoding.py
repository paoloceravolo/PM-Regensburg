# -*- coding: utf-8 -*-
"""4.2TraceEncoding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZWZtiU6QJv-b69aQGa_6lG454o1UI12S
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %pip install pm4py
import pm4py

# process mining 
from pm4py.algo.discovery.alpha import algorithm as alpha_miner
from pm4py.algo.discovery.inductive import algorithm as inductive_miner
from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner
from pm4py.algo.discovery.dfg import algorithm as dfg_discovery

# viz
from pm4py.visualization.petrinet import visualizer as pn_visualizer
from pm4py.visualization.process_tree import visualizer as pt_visualizer
from pm4py.visualization.heuristics_net import visualizer as hn_visualizer
from pm4py.visualization.dfg import visualizer as dfg_visualization

# misc 
from pm4py.objects.conversion.process_tree import converter as pt_converter

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

from pm4py.objects.log.util import dataframe_utils
from pm4py.objects.conversion.log import converter as log_converter

log_csv = pd.read_csv('https://raw.githubusercontent.com/paoloceravolo/PM-Regensburg/main/receipt.csv', sep=',')
parameters = {log_converter.Variants.TO_EVENT_LOG.value.Parameters.CASE_ID_KEY: 'case:concept:name'} # identify the case_id_key name (if not change it will simply be the mane of the coloumn)
event_log = log_converter.apply(log_csv, parameters=parameters, variant=log_converter.Variants.TO_EVENT_LOG)

"""**INDUCTIVE MINER**

The primary idea behind Inductive Miner is to find a base case by detecting a 'cut' in the log (e.g. sequential cut, parallel cut, concurrent cut, and loop cut) and then recurring on sublogs that were found applying the cut. The Directly-Follows variation employs the Directly Follows graph instead of recursion on the sublogs. Hidden transitions are commonly used in inductive mining models, especially for skipping/looping on a piece of the model. In addition, each visible transition gets its own label (there are no transitions in the model that share the same label).WE also possible to convert a process tree into a petri net.
"""

tree = inductive_miner.apply_tree(event_log)
net, im, fm = pt_converter.apply(tree, variant=pt_converter.Variants.TO_PETRI_NET) ##convert a process tree into a petri net.

gviz = pt_visualizer.apply(tree)
pt_visualizer.view(gviz)

gviz = pn_visualizer.apply(net, im, fm)
pn_visualizer.view(gviz)

"""**Feature Selection**

The event log can be represented in a tabular format using a feature selection process. This is critical for tasks such as anomaly detection and prediction.
PM4PY provide a method to transform an event log into a vecrtor using one-hot-encoding. 

One hot encoding is a process by which categorical variables are converted into a a vector of binary values. This vector is suitable to be provided to ML algorithms. So, we see that we have different features for different values of the attribute. This is called one-hot encoding. Actually, a case is assigned to 0 if it does not contain an event with the given value for the attribute; a case is assigned to 1 if it contains at least one event with the attribute. The Pm4PY version also include 2-grams in the lsit of features.
"""

from pm4py.algo.transformation.log_to_features import algorithm as log_to_features

data, feature_names = log_to_features.apply(event_log)
print(feature_names)

df = pd.DataFrame(data, columns=feature_names)
df = df.drop(['trace:enddate','trace:group'], 1)
#df

"""**PCA – Reducing the number of features**

If the dataset's dimensionality is too high, some approaches (such as clustering, prediction, and anomaly detection) suffer. As a result, a dimensionality reduction technique (such as PCA) aids in dealing with the data's complexity. Using techniques like PCA, you may reduce the amount of features in a Pandas dataframe made up of features retrieved from the log. Let's make a PCA with a total number of components of 5 and apply it to the dataframe.
"""

from sklearn.decomposition import PCA

pca = PCA(n_components=3)
df2 = pd.DataFrame(pca.fit_transform(df))
#df2

"""**Decision tree about the ending activity of a process**

Decision trees are objects that help the understandement of the conditions leading to a particular outcome. In this section, several examples related to the construction of the decision trees are provided.
"""

from pm4py.objects.log.util import get_class_representation
target, classes = get_class_representation.get_class_representation_by_str_ev_attr_value_value(event_log, "concept:name")
#target, classes = get_class_representation.get_class_representation_by_trace_duration(event_log, 2 * 8640000)


feature_names = list(df.columns.values)

from sklearn import tree
clf = tree.DecisionTreeClassifier()
clf.fit(df, target)

from pm4py.visualization.decisiontree import visualizer as dectree_visualizer
gviz = dectree_visualizer.apply(clf, feature_names, classes)
#gviz

"""**Cluster Analysis**

As the data we have is based on the characteristics of different types of animals, we can classify animals into different groups(clusters) or subgroups using some well known clustering techniques namely KMeans clustering, DBscan, Hierarchical clustering & KNN(K-Nearest Neighbours) clustering. For sake of simplicity, KMeans clustering ought to be a better option in this case. 

The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.

The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.
"""

# K-MEANS CLUSTERING
from sklearn.cluster import KMeans
from sklearn.cluster import AffinityPropagation
from sklearn.cluster import DBSCAN
from sklearn import metrics
  
#clustering = KMeans(n_clusters = 15)
#clustering = AffinityPropagation(random_state = clusters)
clustering = DBSCAN(eps=0.8, min_samples=3)
clustering.fit(df)
  
print("Class assigned to each point", clustering.labels_)
print("Number of points", len(clustering.labels_))
#print(clustering.inertia_)
print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(df, clustering.labels_))

"""To perform EDA analysis, we need to reduce dimensionality of multivariate data we have to trivariate/bivairate(2D/3D) data. We can achieve this task using PCA(Principal Component Analysis)."""

from matplotlib import colors as mcolors
import math
   
''' Generating different colors in ascending order 
                                of their hsv values '''
colors = list(zip(*sorted((
                    tuple(mcolors.rgb_to_hsv(
                          mcolors.to_rgba(color)[:3])), name)
                     for name, color in dict(
                            mcolors.BASE_COLORS, **mcolors.CSS4_COLORS
                                                      ).items())))[1]
   
   
# number of steps to taken generate n(clusters) colors 
skips = math.floor(len(colors[0 : -4])/clusters)
cluster_colors = colors[0 : -4 : skips]


from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
   
fig = plt.figure()
ax = fig.add_subplot(111, projection = '3d')
ax.scatter(df2[0], df2[1], df2[2], 
           c = list(map(lambda label : cluster_colors[label],
                                            kmeans.labels_)))
   
str_labels = list(map(lambda label:'% s' % label, kmeans.labels_))
   
list(map(lambda data1, data2, data3, str_label:
        ax.text(data1, data2, data3, s = str_label, size = 6.5,
        zorder = 20, color = 'k'), df2[0], df2[1],
        df2[2], str_labels))
   
plt.show()

"""Closely analysing the scatter plot can lead to hypothesis that the clusters formed using the initial data doesn’t have good enough explanatory power. To solve this issue, we need to bring down our set of features to a more useful set of features using which we can generate useful clusters. One way of producing such a set of features is to carry out correlation analysis. This can be done by plotting heatmaps and trisurface plots as follows:"""

import seaborn as sns
  
# generating correlation heatmap
sns.heatmap(df2.corr(), annot = True)
  
# posting correlation heatmap to output console 
plt.show()